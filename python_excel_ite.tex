%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Author style for INFORMS Transactions on Education (ited)
%% Mirko Janc, Ph.D., INFORMS, pubtech@informs.org
%% ver. 0.92, June 2009 -- default options: single-spaced, double-blinded
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[ited]{informs3}                      % for a regular run
%\documentclass[ited,nonblindrev]{informs3}          % for review, not blinded
%\documentclass[ited,blindrev,copyedit]{informs3}    % spaced for copyediting
\documentclass[ited,blindrev]{informs3}              % for review, blinded

\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentstyle. For example
%\documentclass[dvips,ited]{informs1}      % if dvips is used 
%\documentclass[dvipsone,ited]{informs1}   % if dvipsone is used, etc. 

% Private macros here (check that there is no clash with the style)
\newcommand{\code}[1]{\texttt{#1}}

\definecolor{LightGray}{gray}{0.9}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{LightGray},   
    commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegreen},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

%% Setup of theorem styles. Outcomment only one. 
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% In the reviewing and copyediting stage enter the manuscript number.
%\MANUSCRIPTNO{} % When the article is logged in and DOI assigned to it,
                 %   this manuscript number is no longer necessary

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and 
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
\RUNAUTHOR{Isken}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Python for spreadsheet modeling}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{Python for spreadsheet modeling}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows, 
%   should be entered in ONE field, separated by a comma. 
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{Mark W. Isken}
\AFF{Oakland University, \EMAIL{isken@oakland.edu}, \URL{http://www.sba.oakland.edu/faculty/isken/}}
% Enter all authors
} % end of the block

\ABSTRACT{%
TODO: Text of your abstract % Enter your abstract
}%

% Sample
%\KEYWORDS{deterministic inventory theory; infinite linear programming duality; 
%  existence of optimal policies; semi-Markov decision process; cyclic schedule}

% Fill in data. If unknown, outcomment the field
\KEYWORDS{python, spreadsheet modeling}
\HISTORY{}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Samples of sectioning (and labeling) in ITED
% NOTE: (1) \section and \subsection do NOT end with a period
%       (2) \subsubsection and lower need end punctuation
%       (3) capitalization is as shown (title style).
%
%\section{Introduction.}\label{intro} %%1.
%\subsection{Duality and the Classical EOQ Problem.}\label{class-EOQ} %% 1.1.
%\subsection{Outline.}\label{outline1} %% 1.2.
%\subsubsection{Cyclic Schedules for the General Deterministic SMDP.}
%  \label{cyclic-schedules} %% 1.2.1
%\section{Problem Description.}\label{problemdescription} %% 2.

% Text of your paper here

\section{Introduction}

% The opening part of the what-if notebook has some language for the motivation behind this module.
Spreadsheet based modeling wave of the early 2000s. Business schools with business analytics programs are the context. Grad and undergrad.

Why I created 

- more topics I wanted to cover than were possible in PCDA course
- weave in more SE type material which is often overlooked in BA programs

Excel is widely used for building and using models of business problems to explore the impact of various model inputs on key outputs. Built in "what if?" tools such as Excel [Data Tables](https://support.microsoft.com/en-us/office/calculate-multiple-results-by-using-a-data-table-e95e2487-6ca6-4413-ad12-77542a5ea50b) and [Goal Seek](https://support.microsoft.com/en-us/office/use-goal-seek-to-find-the-result-you-want-by-adjusting-an-input-value-320cb99e-f4a4-417f-b1c3-4f369d6e66c7) are well known to power spreadsheet modelers. How might we do similar modeling and analysis using Python?

Limitations of spreadsheets such as reproducibility and automation.


While Python has been gaining momentum in the business analytics world, it is often used for data wrangling, analysis and visualization of tablular data using tools like pandas and matplotlib or Seaborn. You can find some great examples at [Chris Moffit's Practical Business Python blog](https://pbpython.com/). I use Python all the time for such tasks and teach a course called [Practical Computing for Data Analytics](http://www.sba.oakland.edu/faculty/isken/courses/mis5470/) that is Python (and R) based. But, it got me to thinking. What about those things for which Excel is well suited such as building formula based models and doing sensitivity analysis on these models? What would those look like in Python?

Quant Econ example

PBB blog


Teach relevant Python to business students using familiar and relevant modeling examples.
Give sense of software development process and design decisions
Model a learning journey
Teach students to think and act like a software developer, not just an analyst

\section{Module positioning and structure}

PCDA ends with intro to sklearn and AAP begins with review of sklearn and some more ML content - logistic regression with regularization

What kind of Python background do students need to have in order to be ready for this EwP module? This module does presume a basic familiarity with Python fundamentals for data analysis such as:

\begin{itemize}
	\item variables, artithmetic and boolean operators, and basic data types,
	\item data structures such as lists, dictionaries, tuples, NumPy arrays and pandas dataframes,
	\item flow control such as branching with \texttt{if \ldots else} and looping with \texttt{for} and \texttt{while}
	\item module imports,
	\item using built in functions and accessing methods and properties of objects,
	\item creating functions,
	\item basic use of NumPy and pandas for data wrangling and analysis,
	\item basic plotting with matplotlib.
\end{itemize}

The EwP module could easily be used as part of a semester long Python based analytics course. At our institution, this module is part of a business course entitled "Advanced Analytics with Python" (\href{http://www.sba.oakland.edu/faculty/isken/courses/mis6900}{AAP}). The students taking this course have already taken my "Practical Computing for Business Analytics" (\href{http://www.sba.oakland.edu/faculty/isken/courses/mis5470}{PCDA}) course in which they spend about half the semester learning R and the other half learning Python, both in the context of business analytics. Some Linux basics are also part of the PCDA course. The students have also already taken "Business Analytics" (\href{http://www.sba.oakland.edu/faculty/isken/courses/mis5460}{BA}) which is a spreadsheet based introduction to business analytics. The BA and PCDA courses have historically been offered in face to face mode in a computer teaching lab. Since 2020, these two courses have been also offered as online, asynchronous courses. The newer AAP course has been offered since 2021 in an online asynchronous mode. All three courses have extensive course websites that are publicly accessible, including access to all of the video content and files needed. We will describe the AAP course web in more detail below in the context of the EwP module.

The EwP module is made of three submodules. The first of these focuses on building and using a typical spreadsheet model using Python. Then the functionality developed for doing this type of modeling in Python is deployed as a reusable package. We end with an introduction to using Python to manipulate Excel files, an extremely common use case.

Combination of Jupyter notebooks, Python scripts, videos walking through the notebooks and scripts, all publicly accessible from submodule specific course web pages. GIVE URL. Some background on Jupyter notebooks for analytics. Questions and coding challenges sprinkled throughout the notebooks - answers at bottom. While AAP is online asynch, this same approach has been used very successfully in a computer teaching lab for the PCDA course. The screencasts are highly valued by the students as they can rewatch things as needed. Many are working professionals and a 2.5 hour evening class in a warm computer lab can be taxing.

\section{Submodule 1: What-if analysis with Python}

There are two overarching learning objectives for this first set of Jupyter notebooks. Students learn how Python might be used to do something they are quite familiar with doing in a spreadsheet - model building and sensitivity analysis. At the same time, students encounter, as they are needed, more advanced Python programming techniques such as object oriented programming and manipulation of tuples, lists and dictionaries. In this way, the computational problem to be solved drives the introduction of more advanced Python concepts and techniques instead of such things being presented in a vacuum. I make it a point to try to convey the notion that there are often multiple possible approaches by embarking on one path and then starting over and doing things in a different way. Each notebook is accompanied by a few to several videos that walk the students through the material - a guided tour with challenges to overcome along the way. The screenshot in \ref{fig:activities} shows the portion of the course web page that provides the high level roadmap through the notebooks and associated screencasts.
 
 
\subsection{Notebook 1: Modeling and data tables}

\begin{quote}
	For example, here's a high level screenshot of a model that I assign for homework in my [MIS 4460/5460 Business Analytics class](http://www.sba.oakland.edu/faculty/isken/courses/mis5460/) (a spreadsheet based modeling class).
	It's a really simple model in which we are selling a single product that we produce. There is a fixed cost to producing the product as well as a variable production cost per unit. We can sell the product for some price and we believe that demand for the product is related to the selling price through a power function. Let's assume for now that we have sufficient capacity to produce to demand and that all inputs are deterministic (we'll deal with simulating uncertainty later in this document).
	
	The details aren't so important right now as is the overall structure of the model. There's a few key inputs and some pretty straightforward formulas for computing cost, revenue and profit. Notice the 1-way Data Table being used to explore how profit varies for different selling prices. There's a graph driven by the Data Table and some Text Boxes used for annotation and summary interpretative comments. There's a button that launches Goal Seek to find the break even selling price and a 2-way Data Table (not shown) to explore the joint effect of selling price and variable cost. Classic Excel modeling stuff. How might we go about building a similar model using Python?  
	
	What if we wanted to push it a little further and model some of the key inputs with probability distributions to reflect our uncertainty about their values? In the Excel world, we might use add-ins such as @Risk which allow uncertain quantities to be directly modeled with probability distributions. For example, we might have a key input such as the exponent in the power function that relates selling price to demand that is highly uncertain. By modeling it with a probability distribution and then sampling from that distribution many times (essentially by recalcing the spreadsheet) we can generate a bunch of possible values for key outputs (e.g. profit) and use statistics to summarize these outputs using things like histograms and summary stats. Often this type of simulation model is referred to as a *Monte-Carlo* model to suggest repeated sampling from one or more probability distributions within an otherwise pretty static model. If you want to see such models in action, check out my [Simulation Modeling with Excel page](http://www.sba.oakland.edu/faculty/isken/courses/mis5460/simulation.html) from my Business Analytics course. Again, how might we do this with Python?
\end{quote}


The first focuses on building a simple what-if model and doing sensitivity analysis much like one would do using Excel's Data-Table functionality.

This example is based on one in the [spreadsheet modeling textbooks I've used in my classes since 2001](https://host.kelley.iu.edu/albrightbooks/). In both books, they introduce the "Walton Bookstore" problem in the chapter on Monte-Carlo simulation. Here is the basic problem (with a few modifications):

\begin{itemize}
	\item we have to place an order for a perishable product (e.g. a calendar),
	\item there is a known unit cost for each one ordered,
	\item we have a known selling price,
	\item demand is uncertain but we can model it with some simple probability distribution,
	\item for each unsold item, we can get a partial refund of our unit cost,
	\item we need to select the order quantity for our one order for the year; orders can only be in multiples of 25.
\end{itemize}

\subsubsection{Build first model - procedural approach}
 
The notebook begins with a suggestion to start with a very simple model that ignores the uncertainty in demand and that is not object oriented. Proceeding much like we would in Excel, we store base input values in variables. 

\begin{lstlisting}[language=Python]
# Base inputs
unit_cost = 7.50
selling_price = 10.00
unit_refund = 2.50

# Demand parameters
demand_mean = 193
demand_sd = 40

# Deterministic model
demand = demand_mean

# Initial value for order quantity - this is the decision variable
order_quantity = 200
\end{lstlisting}

Then students are given some skeleton code and must finish the expressions for computing key intermediate outputs (\code{order_cost, sales_revenue, refund_revenue} and the final output, \code{profit}. Answers are provided at the bottom of the notebook so that students can attempt to finish the code but have a resource to correctly complete the code and move on. The screencast associated with the notebook also shows me completing the code. Using skeleton code in this way has worked well both for face to face lab based versions of this type of course as well as for online asynchronous versions.

\begin{lstlisting}[language=Python]
order_cost = unit_cost * order_quantity
sales_revenue = ??? * selling_price
refund_revenue = ??? * unit_refund
profit = sales_revenue + refund_revenue - order_cost
\end{lstlisting}

At this point we discuss a fundamental difference between the computing model of Excel and that of Jupyter notebooks. While spreadsheets respond automatically to changes, Jupyter notebooks require rerunning all code cells that include or follow the changed code.

\subsubsection{Sensitivity analysis}

Since order quantity is the key decision variable, we might want to see how profit changes for different order quantities in this first simplified model. In Excel, the Data Table tool provides an easy way to do this and is a staple of spreadsheet based management science textbooks. In a Data Table, a range of order quantities can be used as row or column input and one or more output values can be computed. In Python, we can rely on the built in vectorized behavior of the language. A range of order quantities is created as a NumPy array and then can be used directly to compute vectors of all of the intermediate and final output variables. Much like Data Tables, vectorized computations avoid having to explicitly iterate (loop) through a collection of input values.

A natural next step is to do the equivalent of an Excel 2-way Data Table. Can't do 2 vectors since we need all combinations of entries in each vector. List comprehensions provide a compact way to create nested loops over the two vectors. By creating a profit function that takes all of the base inputs as arguments, we can construct a list comprehension that does what an Excel 2-Way Data Table does. Even better, this actually allows us to do the equivalent of an n-Way Data Table.

\begin{lstlisting}[language=Python]
	def bookstore_profit(unit_cost, selling_price, unit_refund, order_quantity, demand):
	'''
	Compute profit in bookstore model
	'''
		order_cost = unit_cost * order_quantity
		sales_revenue = np.minimum(order_quantity, demand) * selling_price
		refund_revenue = np.maximum(0, order_quantity - demand)
		profit = sales_revenue + refund_revenue - order_cost
		return profit
	
	# Set up input vector ranges	
	demand_range = np.arange(50, 301, 5)
	order_quantity_range = np.arange(50, 301, 25)
	
	# Create data table (as a list of tuples)
	data_table_1 = [(d, oq, bookstore_profit(unit_cost, selling_price, unit_refund, oq, d)) 
	for d in demand_range for oq in order_quantity_range]
	
	# Convert to dataframe
	dtbl_1_df = pd.DataFrame(data_table_1, columns=['Demand', 'OrderQuantity', 'Profit'])
	
\end{lstlisting}

TODO - add output listing and plot

\subsubsection{The object nature of Python}

Introducing basic OO concepts is a primary learning objective. It's not clear whether an OO or a non-OO approach will end up making the most sense for doing spreadsheet type modeling. This uncertainty is made explicit in the notebook and students are led on a bit of a journey of discovery as we explore different software designs for our modeling problem. I purposefully designed the notebooks in this way to show students that the path to a solution is not necessarily linear and that giving yourself the freedom to explore alternatives, many of which will be dead ends, can lead to a rich learning experience. It is also more realistic in the sense that this is how most real problems are solved. It's analogous to the idea that mathematical proofs usually don't reflect the myriad of failed approaches that were tried but that ultimately contributed to the final product.

In preparation for building an object oriented version of the model, basic object concepts are reviewed. Since most students have done some Excel VBA programming in a previous course, I refer back to a few properties and methods of the Excel \code{Worksheet} object. In Python, everything is an object. Using Python lists as an example, we explore some of its attributes such as methods for appending items to a list and reversing the order of its elements. The \code{dir} function is used to see all of an object's attributes and we see that even things like integers are objects in Python. With this basic object refresher, we are ready to do some actual object oriented programming and create our own objects - well, actually classes.

\subsubsection{Creating an object oriented model}

For our initial design of a \code{BookstoreModel} class we make all of the base inputs class attributes (properties) and then add method attributes to compute outputs such as costs, revenues and profits. Students can easily see the mapping between things in the initial procedural model and this new object oriented version of the model. I highlight several important concepts and syntactical details of doing OO programming Python. Again, the goal is to weave in these more advanced Python programming concepts within the context of a very familiar modeling problem. This section of the notebook ends with us using our \code{BookstoreModel} class to create a model object with all of its base inputs instantiated with values. Before moving on students are presented with a few challenges involving enhancements to the \code{BookstoreModel} class. Interspersing short coding challenges throughout the notebook gives students a chance to test themselves on their understanding and to get some coding practice.

Then a few design dilemmas are posed and some non-working approaches illustrated as we try to decide on how to implement an n-way data table function for this new object oriented model. These meta reflections are intentionally included to better mimic actual software development and reinforce the idea that software development is much more than simply writing code to implement a perfectly thought out design. We forge ahead with some preliminary ideas and are confronted with an intermediate problem related to generating a list of dictionaries that represent combinations of inputs (i.e. scenarios) to evaluate by our n-way data table function. This provides a great opportunity to remind students of one of the great strengths of the open source software ecosystem - we can leverage work done in other packages and even look at actual source code to see how something was implemented. In this case, the well known scikit-learn package has a \code{ParameterGrid} function that solves our scenario generation problem quite nicely. This example also reminds the students of the value of being able to make sense of API documentation and of object oriented code written by others. Most data science related Python packages are object oriented in nature. 

Whew! This first notebook draws to a close with the creation of an \code{data_table} function that allows us to do sensitivity analysis with our objected oriented model. Unlike Excel's Data Table tool, our \code{data_table} function can handle an arbitrary number of inputs and outputs. The payoff is the creation of the following faceted plot showing how profit varies for different order quantities and demand levels. We also recap the main things we learned in this notebook and reiterate that we will now move on to adding goal seeking capability to our model.

INSERT PLOT HERE


\subsection{Notebook 2: Goal seek}

No self respecting modeling tool would be complete without goal seeking capability to do things such as finding the break even point in the bookstore model. For many students, Excel's Goal Seek tool can seem almost magical when encountered for the first time. Even after they get a sense of what it is doing, understanding the importance of the initial guess and how it can lead to Goal Seek reporting different solutions, is not very transparent. With Python, students can gain a much better understanding of how tools like Goal Seek really work. 

Before trying to build our \code{goal_seek} function we take a brief detour into basic root finding algorithms through packages like SciPy as well as in standalone Python scripts that are widely available in the Python data science ecosystem (refs). We compare the output of Excel's Goal Seek with different starting values to the values obtained by different root finding algorithms implemented in Python. The importance of starting values and also the range of different root finding algorithms available is often surprising to business students who have spent most of their life in Excel.

A \code{} function is created that implements a simple bisection search and we use it to find the break even demand point in our bookstore model. With \code{data_table} and \code{goal_seek} functions implemented, we are ready to move on to doing Monte-Carlo simulation.

\subsection{Notebook 3: Monte-Carlo simulation}

In the Excel modeling world, Monte-Carlo simulation can be done without add-ins, but packages like @Risk \cite{} can make the process much easier. In particular, random variable generation for a wide range of probability distributions is facilitated by @Risk. In Python, the NumPy package provide random variate generation for a large number of distributions (https://numpy.org/doc/stable/reference/random/legacy.html#distributions) and similarly, SciPy provides functions for computing distribution related quantities for many probability distributions \cite{}. Much like we did when creating the \code{data_table} function, we start with adding uncertainty to a single input variable in the bookstore model and rely on Python's vectorized computing capabilities to generate the profit associated with each realization of the random input. Then we can do standard statistical analysis of the simulation output using Python packages such as Pandas and SciPy. In spreadsheet based simulation textbooks, the notion of the ``Flaw of Averages'' popularized by Sam Savage \cite{} is illustrated. We do the same with our Python based model, showing how replacing random demand by its mean can result in a wildly optimistic estimate of the mean profit found through analyzing the simulation output. Much like doing simulation in Excel without add-ins, using Python makes transparent the quite simple and brute force nature of Monte-Carlo simulation.

Moving on to multiple uncertain inputs raises challenges much like those we faced when moving from a 1-way to an n-way data table function. In addition to modeling multiple uncertain inputs, we want to be able to specify a range of scenarios to run, much like we did for the \code{data_table} function. The following set of design criteria are presented for a \code{simulate} function.

\begin{itemize}
	\tightlist
	\item
	The first argument will be a model object (i.e.~something like the
	\texttt{BookstoreModel} model) that contains an \texttt{update}
	method. Soon, we should add an abstract \texttt{Model} class from
	which specific models such as the \texttt{BookstoreModel} class can be
	created. The abstract class will contain the \texttt{update} method.
	\item
	The random inputs will be passed in as a dictionary whose keys are the
	input variables being modeled as random and whose values are an
	iterable representing the draws from some probability distribution.
	Structurally, this is similar to how inputs are specified in the
	\texttt{data\_table} function.
	\item
	We can optionally pass in a dictionary of scenario inputs. This is
	exactly like the \texttt{data\_table} variable input.
	
	\begin{itemize}
		\tightlist
		\item
		If no scenario input dictionary is passed in, a single simulation
		scenario is run using the current input values in the model object,
		\item
		If a scenario input dictionary is passed in, then a simulation
		scenario is run for every combination of parameters in the
		dictionary. Again, this is just like we do in the
		\texttt{data\_table} function.
	\end{itemize}
	\item
	The output will be a set of dictionaries containing dataframes of
	simulation output as well standard summary stats and plots.
\end{itemize}

This is followed by a well commented first attempt at a \code{simulate} function. By this point, students are well equipped to understand the code as it builds on basic Python concepts and more advanced things that we have already covered in this EwP module. The rest of the notebook is spent trying out the \code{simulate} function and ends with creating a plot that many of them have likely created before with @Risk along with a faceted plot of histograms of profit created with the popular Seaborn package.

INSERT GROUPED BOX PLOT HERE and FACETED HISTOGRAMS

The first submodule and its three notebooks are very much about the type of modeling typically done in a spreadsheet based course. In a spreadsheet based course, students learn numerous Excel functions and techniques in the process of building and exercising such models. Similarly, numerous Python techniques are scattered throughout the three modeling focused notebooks. Now we shift gears and move beyond what might be considered prototyping and into the realm of deployment. 

\section{Submodule 2: The packaging notebooks}



\begin{quote}
	content...

In the first three notebooks, we've developed some Python approaches to
typical Excel ``what if?'' analyses. Along the way we explored some
slightly more advanced Python topics (for relative newcomers to Python)
such as:

\begin{itemize}
	\tightlist
	\item
	List comprehensions,
	\item
	Basic OO programming, creating our own classes and using things like
	\texttt{setattr} and \texttt{getattr},
	\item
	Leveraging scikit-learn's \texttt{ParameterGrid} class,
	\item
	Faceted plots using Seaborn and matplotlib,
	\item
	Tuple unpacking, zip, and itertools,
	\item
	Safely copying objects,
	\item
	Root finding with, and without, \texttt{scipy.optimize},
	\item
	Using \texttt{numpy.random} to generate random variates from various
	probability distributions,
	\item
	Using \texttt{scipy.stats} to compute probabilities and percentiles,
	\item
	Importing our \texttt{data\_table}, \texttt{goal\_seek} and
	\texttt{simulate} functions from a module.
\end{itemize}

Now that we've got a critical mass of "proof of concept" code, let's figure out how to structure our project and create a deployable package. In addition, let's rethink our OO design and add some much needed documentation to the code.
\end{quote}

Creating a deployable Python package is a good exercise in designing and creating reusable software artifacts. Thinking about how others will interact with your software usually leads to better designed software. Basic software engineering practices are often overlooked in business analytics programs but are certainly desirable in the professional world. In this submodule we create a Python package that can be imported and used by other modelers. This will require some design changes to our software and will also involve moving code out of Jupyter notebooks and into Python script files. While Jupyter notebooks can be integrated into production environments \cite{netflix}, it is much more common for code to live in a Python script file. In addition to creating a deployable package, this submodule also discusses the importance of documentation in its many forms in a Python project.

\subsection{Notebook 4: Project packaging}

The first notebook in this submodule starts by explaining what Python package are and their role in sharing code. We then revisit a concept from earlier in the course - creating a good project folder structure. For this we use what is known as a \textit{cookiecutter} that can automatically generate a project folder structure and key files from the answers to a few prompts. Their are a few different folder layouts used in Python projects and there is a bit of discussion on the folder stucture we will use and resources given for exploring this issue further. Students are reminded of the importance of version control and we initialize a git repository for our budding package. The basics of version control were already covered in the first week of the course. My experience is that most students have little exposure to simple software project management concepts such as project folder structures and version control. This can lead to a jumble of files with incoherent names used as a proxy for version control - whatif_v1.py, whatif_v2.py, whatif_final.py, whatif_finalfinal.py, ...

Key software design changes are then made and discussed which will make our code easier on our users. These changes include creating an abstract \code{Model} base class from which different model classes can be created and moving the \code{data_table}, \code{goal_seek}, and \code{simulate} functions into the \code{Model} base class. All of the code is moved into a code module named \code{whatif.py}. We are now ready to learn about the basics of turning our code into an installable package. This requires a basic understanding of two key Python files that are ubiquitous in Python pacakges - \code{__init__.py} and \code{setup.py}. Next comes a discussion of options for installing our package locally so that we can use it for different modeling projects. This includes an explanation of how and where Python searches for packages when \code{import} statements are encountered in code. We defer discussion of uploading our package to a public code sharing platform such as PyPI. All of this can be quite complex and I have made every effort to distill things down to a minimal level of complexity needed to convey the important points. I stress that there is much complexity lurking here and it is something they will need to revisit time and time again.

This notebook ends by pointing the students to another notebook in which our newly deployed whatif package is used for a completely different model. The example used is the New Car Simulation that has been a staple of Winston and Albright's textbooks for many years \cite{bibid}.

\subsection{Notebook 5: Documentation}

This deployment focused submodule ends with a short notebook discussing the different types of documentation needed in a typical project. This includes things like code comments, docstrings, readme files and generating documentation by writing restrutured text files and using Sphinx. This is one of those tasks that few people like to do but can be very important for the long term success of a project. Even if you are just creating a tool for yourself, you will forget the details over time and documentation can be very beneficial in refreshing your memory. 

\section{Submodule 3: Excel data wrangling with Python}

The EwP module ends with what has become a relatively common use of Python with Excel - automating various Excel data wrangling tasks. Examples include:

\begin{itemize}
	\tightlist
	\item
	You have a whole folder full of csv (or Excel) files with the same
	file structure and you need to combine them into a single file. You
	might also need to make some changes to the consolidated file.
	\item
	You have an Excel file with multiple sheets of similarly structured
	data and you want to consolidate them into a single sheet.
	\item
	You have an Excel file with data in ``wide'' format and you need to
	convert it to long format, and then perhaps export out individual
	files (one per the key column(s) in the long formatted data).
	\item
	You have an Excel file acting as a simple flatfile database.
	Periodically, you get new Excel files that need to get appended to the
	``database'' file.
\end{itemize}

A single Jupyter notebook is used to give a taste of using Python to automate the process of working with Excel files by tackling each of the four examples above. Each of these examples are based on real problems I encountered either in research or industrial projects. TRANSITION NEEDED Several web based resources are shared with one particularly good one being the Practical Business Python blog written by Chris Moffitt. It includes numerous posts on different aspects of Python and Excel integration. There are a number of Python based tools that either include functionality for working with Excel files or are dedicated to specific Excel related operations. These include:

\begin{itemize}
	\tightlist
	\item
	pandas -
	\href{https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html}{read\_excel},
	\href{https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html}{ExcelWriter},
	\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html}{to\_excel}
	\item
	\href{https://openpyxl.readthedocs.io/en/stable/}{openpyxl} - can
	read, write, and modify Excel files
	\item
	\href{https://xlsxwriter.readthedocs.io/}{XlsxWriter} - powerful but
	can't edit existing Excel files
	https://xlsxwriter.readthedocs.io/working\_with\_tables.html
	\item
	\href{https://www.xlwings.org/}{xlwings} - provides ``glue'' between
	Python and Excel
\end{itemize}

\subsection{Example 1: Concatenating many csv files}

While this first example is not necessarily Excel related, using Excel to consolidate csv files through a series of copy-paste operations is something that many Excel users will admit to doing. We also use this example to introduce the newish pathlib Python library for working with file paths. After getting familiar with pathlib, students are led through the development of a short procedure that creates a list of the csv files to concatenate and then uses the pandas library to read and combine the files in a \code{DataFrame} and then writes out the consolidated data as a csv file.

\subsection{Example 2: Consolidating data from multiple sheets in a Excel file using pandas}

Two different approaches are shown to consolidate data from multiple sheets into a single sheet. One approach uses the pandas library while the other relies on a library specifically created for working with Excel files from Python, openpyxl. One advantage to using Python rather than Visual Basic for Applications for this task is that Python is well suited for working with data in many different formats. The Excel file may be just one data source out of a collection of data sources that are used to create some sort of combined database. The generality of Python is a strength in such scenarios.

\subsection{Example 3: Dealing with wide data and a multi-row header}

Reshaping data from wide to long is a common data wrangling task especially in preparation for plotting. In this example, not only do students have to grapple with reshaping the data in an Excel workbook, they also have to deal with the all too common problem of multi-row header lines in a spreadsheet. Excel's flexibility makes it easy to create such multi-row headers and pity the poor analyst who then has to deal with it when trying to move the data into some sort of data frame or database table. This example is posed as a challenge to the students. Examples of the desired outputs are shown and a few hints given. The answer is provided at the end of the notebook. This example really shows the power of Python in creating a reproducible and automated approach to dealing with a poorly structured Excel workbook, creating not only restructured data frames but also non-trivial plots. Attempting to do all of this in Excel, even with VBA, is not an easy task.

\subsection{Example 4: Appending new spreadsheet data in consolidated Excel workbook}

Excel is not a database. But, that does not stop people from using it as a simple flat-file database. In such a scenario, it is not uncommon to have to periodically append new data to the end of a range of data in an Excel workbook. Again, a combination of openpyxl and pandas allows us to meet this challenge.

\subsection{More on Python/Excel integration}

The examples above barely scratch the surface of the use cases for Python integration with Excel. Our notebook ends with a shout out to the highly regarding Practical Business with Python blog and a list of specific examples that students can explore on their own.

\begin{itemize}
	\tightlist
	\item
	\href{https://pbpython.com/excel-file-combine.html}{Combining Data
		From Multiple Excel Files} - file globbing, concatenating dataframes,
	\item
	\href{https://pbpython.com/pandas-excel-range.html}{Reading Poorly
		Structured Excel Files with Pandas} - advanced use of
	\texttt{read\_excel}, accessing ranges and Tables
	\item
	\href{https://pbpython.com/excel-pandas-comp.html}{Common Excel Tasks
		Demonstrated in Pandas} - totals rows, fuzzy string matching
	\item
	\href{https://pbpython.com/excel-pandas-comp-2.html}{Common Excel
		Tasks Demonstrated in Pandas - Part 2} - selection and filtering
	\item
	\href{https://pbpython.com/improve-pandas-excel-output.html}{Improving
		pandas Excel output} - using XlsxWriter to format Excel workbooks from
	Python
	\item
	\href{https://pbpython.com/advanced-excel-workbooks.html}{Creating
		Advanced Excel Workbooks} - XlsxWriter, inserting VBA from Python(!),
	using COM to merge sheets
	\item
	\href{https://pbpython.com/xlwings-pandas-excel.html}{Interactive Data
		Analysis with Python and Excel} - using xlwings to ``glue'' Python and
	Excel together, using sqlalchemy to interact with databases
\end{itemize}

\section{Classroom experience}

\section{Random notes}
Learning good Python programming practices via learning to model in similar way that students learn good spreadsheet practices through building spreadsheet models using the W\&A textbook.

How aap course fits in relation to pcda course. Could also be second part of a fully Python based course. Builds on basic intro Python concepts and skills.

The Quant Econ website is a good example of another use of Python for typical Excel things.

Everything discussed in this article is available from the publicly accessible course website.

% Acknowledgments here
\ACKNOWLEDGMENT{%
% Enter the text of acknowledgments here
}% Leave this (end of acknowledgment)


% Appendix here
% Options are (1) APPENDIX (with or without general title) or 
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
% \begin{APPENDIX}{<Title of the Appendix>}
% \end{APPENDIX}
%
%   or 
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}


% References here (outcomment the appropriate case) 

% CASE 1: BiBTeX used to constantly update the references 
%   (while the paper is being written).
%\bibliographystyle{informs2014} % outcomment this and next line in Case 1
%\bibliography{<your bib file(s)>} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

\end{document}


